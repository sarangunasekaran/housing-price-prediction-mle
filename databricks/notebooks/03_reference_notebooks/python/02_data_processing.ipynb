{"cells":[{"cell_type":"markdown","source":["# Purpose"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f87236d0-26ee-4bc2-82a1-c22c0c68b2d0"}}},{"cell_type":"markdown","source":["This notebook demonstrates the data pipeline from raw tables to analytical datasets. At the end of this activity, train & test data sets are created from raw data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2387f7e9-64b2-44ab-865d-8f95bae489e3"}}},{"cell_type":"markdown","source":["## Imports"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65ce6809-a996-43dc-865d-d530653f83d6"}}},{"cell_type":"code","source":["from ta_lib.core.api import get_package_path\nPACKAGE_PATH=get_package_path()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"182bb2ad-4b7d-4658-a3ab-cb8d615d7767"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["import sys\nimport os.path as op\nCURR_LOC_IN_DBFS=op.join(PACKAGE_PATH,\"..\",\"databricks/03_reference_notebooks/python/\")\nsys.path.append(CURR_LOC_IN_DBFS)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8aff804-bcfb-4a12-9e48-06477befbd07"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["from pprint import pprint\nimport os\nimport os.path as op\nimport shutil\n\n# standard third party imports\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\npd.options.mode.use_inf_as_na = True\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71bd4c2a-357e-4a86-8bea-56f794288183"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%load_ext autoreload\n%autoreload 2"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ac30d0e-93f9-4d91-9859-6eb25b225390"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# standard code-template imports\nfrom ta_lib.core.api import (\n    create_context, get_dataframe, get_feature_names_from_column_transformer,\n    display_as_tabs, string_cleaning, merge_info, initialize_environment,\n    list_datasets, load_dataset, save_dataset\n)\nimport ta_lib.eda.api as eda"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7bf0394-f033-4394-814a-fb7a4324abeb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["initialize_environment(debug=False, hide_warnings=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd81fef3-9ae0-48c7-b8d9-e24feaca43c4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Utility functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5149dab-12f0-41ba-a66e-99fc791eefee"}}},{"cell_type":"markdown","source":["# 1. Initialization"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b263eff7-5860-431c-aca4-e63404d905e3"}}},{"cell_type":"code","source":["config_path = op.join(CURR_LOC_IN_DBFS,\"conf\",\"config.yml\")\ncontext = create_context(config_path)\npprint(list_datasets(context))\n\norders_df = load_dataset(context, 'raw/orders')\nprod_df = load_dataset(context, 'raw/product')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"226871df-4be2-42dc-8213-a915989e6619"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# 2. Data cleaning and consolidation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16ca1ef4-b660-4a06-8dce-9010f950265d"}}},{"cell_type":"markdown","source":["**<u>NOTES</u>**\n\nThe focus here is to create a cleaned dataset that is appropriate for solving the DS problem at hand from the raw data.\n\n**1. Do**\n* clean dataframe column names\n* ensure dtypes are set properly\n* join with other tables etc to create features\n* transform, if appropriate, datetime like columns to generate additional features (weekday etc)\n* transform, if appropriate, string columns to generate additional features\n* discard cols that are not useful for training the model (IDs, constant cols, duplicate cols etc)\n* additional features generated from existing columns\n\n\n**2. Don't**\n* handle missing values or outliers here. mark them and leave them for processing downstream."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e3f0cec-c922-44d4-9dc5-03d75d91910d"}}},{"cell_type":"markdown","source":["## 2.1 Clean individual tables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32b0cb3b-0df6-47a7-a28c-1b0baf5b0167"}}},{"cell_type":"markdown","source":["### Products Table\n\nFrom data discovery, we know the following\n\n* all columns are strings : nothing to fix. Apply generic cleaning (strip extra whitespace etc)\n* ensure all `invalid` string entries are mapped to np.NaN\n* some column are duplicates (eg. color, Ext_Color). Better to `coalesce` them instead of an outright discard of one of the columns.\n* SKU is key column : ensure no duplicate values\n* This will go into production code"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3d12e6b-8b81-49a7-ba36-4114409660d4"}}},{"cell_type":"code","source":["prod_df_clean = (\n    prod_df\n    # while iterating on testing, it's good to copy the dataset(or a subset)\n    # as the following steps will mutate the input dataframe. The copy should be\n    # removed in the production code to avoid introducing perf. bottlenecks.\n    .copy()\n\n    # set dtypes : nothing to do here\n    .passthrough()\n\n    .transform_columns(prod_df.columns.to_list(), string_cleaning, elementwise=False)\n    \n    .replace({'': np.NaN})\n    \n    # drop unnecessary cols : nothing to do here\n    .coalesce(['color', 'Ext_Color'], 'color', delete_columns=True)\n    \n    # drop unnecessary cols : nothing to do here\n    .coalesce(['MemorySize', 'Ext_memorySize'], 'memory_size', delete_columns=True)\n    \n    # ensure that the key column does not have duplicate records\n    .remove_duplicate_rows(col_names=['SKU'], keep_first=True)\n    \n    # clean column names (comment out this line while cleaning data above)\n    .clean_names(case_type='snake')\n)\nprod_df_clean.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f704451e-19db-4f4f-a8e7-bcc99cec465d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### NOTE\n\nIt's always a good idea to save cleaned tabular data using a storage format that supports the following \n\n1. preserves the type information\n2. language agnostic storage format\n3. Supports compression\n4. Supports customizing storage to optimize different data access patterns\n\nFor larger datasets, the last two points become crucial.\n\n`Parquet` is one such file format that is very popular for storing tabular data. It has some nice properties:\n- Similar to pickles & RDS datasets, but compatible with all languages\n- Preserves the datatypes\n- Compresses the data and reduces the filesize\n- Good library support in Python and other languages\n- As a columnar storage we can efficiently read fewer columns\n- It also supports chunking data by groups of columns (for instance, by dates or a particular value of a key column) that makes loading subsets of the data fast."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c97439e4-66da-4424-bfce-72e243efb2f6"}}},{"cell_type":"code","source":["save_dataset(context, prod_df_clean, 'cleaned/product')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59f5672b-afa3-40ec-a4ca-402b0f31e274"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Orders Table\n\nFrom data discovery, we know the following\n\n* key columns: None\n* integer columns: Quantity, InvoiceNo, Orderno, Quantity\n* datetime columns: LedgerDate\n* This will go into production code"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0846711d-58e4-46f7-92dc-964692fe44c0"}}},{"cell_type":"code","source":["# column names after cleaning\n\nstr_cols = list(\n    set(orders_df.select_dtypes('object').columns.to_list()) \n    - set(['Customername', 'InvoiceNo','Quantity', 'InvoiceNo', 'Orderno', 'LedgerDate'])\n)\norders_df_clean = (\n    orders_df\n    \n    .copy()\n    #.sample(frac=1, resample=False)\n\n    # set dtypes\n    .change_type(['Quantity', 'InvoiceNo', 'Orderno'], np.int64)\n    \n    # set dtypes\n    .to_datetime('LedgerDate', format='%d/%m/%Y')\n    \n    # clean string columns (NOTE: only after setting datetime)\n    .transform_columns(str_cols, string_cleaning, elementwise=False)\n\n    # clean column names                                                                                                                                   \n    .clean_names(case_type='snake')\n    .rename_columns({'orderno': 'order_no'})\n)\norders_df_clean.head().T\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9db53d2e-dc09-45f4-aa01-d0bc64b45fc5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["save_dataset(context, orders_df_clean, 'cleaned/orders')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03b53d51-9d6e-416e-aa17-31301675a32c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 2.2 Create consolidated features table\n\nHere we take the cleaned dataframes and merge them to form the consolidated table.\n\nWe know from data discovery that `products` is a dimension table and `orders` is a fact table, so we want to do a left join here. * This will go into production code"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"63d010a7-2187-4db8-b34b-a8cdd7812234"}}},{"cell_type":"code","source":["sales_df = pd.merge(orders_df_clean, prod_df_clean, how='inner', on='sku', validate='m:1')\nmerge_info(orders_df_clean, prod_df_clean, sales_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"728d46ee-7439-446e-8875-529148627d41"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 2.3 Business intuition features\n\nThis section can go into production code if these features are used in final model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef3892d0-4bd5-46e3-8b8a-c7595801f7bf"}}},{"cell_type":"markdown","source":["#### First Time Customer.\n- A binary feature that tells if the customer is in business for the first time or not."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed7b8543-771f-4397-beea-95c873b8db10"}}},{"cell_type":"code","source":["# first time customer\ncust_details = sales_df.groupby(['customername']).agg({'ledger_date':'min'}).reset_index()\ncust_details.columns = ['customername','ledger_date']\ncust_details['first_time_customer'] = 1\nsales_df = sales_df.merge(cust_details, on=['customername','ledger_date'], how='left')\nsales_df['first_time_customer'].fillna(0, inplace=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"505ab225-049c-45cc-aba7-502d21b49a53"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Days Since Last Purchase of a customer\n- Feature representing the number of days from the last purchase of a customer. \n- Quantifies the Gaps customers take b/w purchases"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3878a2a3-9ea3-4cc4-beae-9edef8b8be3c"}}},{"cell_type":"code","source":["#### days since last purchase\nsales_df.sort_values('ledger_date',inplace=True)\nsales_df['days_since_last_purchase'] = (\n    sales_df\n       .groupby('customername')['ledger_date']\n       .diff()\n       .dt.days\n       .fillna(0, downcast='infer'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81a227a7-e2fd-483d-ac24-13d370c1967f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# create a sample dataframe with minimal processing\n\nsales_df_processed = (\n    sales_df\n    \n    # tweak to test pipeline quickly or profile performance\n    #.sample(frac=1, replace=False)\n    \n    # any additional processing/cleaning\n)\n\n# Any verifications on the data\nfrom ta_lib.eda.api import get_variable_summary\ndisplayHTML(display_as_tabs([\n    (\"Summary\", f\"Length: {len(sales_df_processed)}, Columns: {len(sales_df_processed.columns)}\"),\n    (\"Variable summary\", get_variable_summary(sales_df_processed)),\n    (\"head\", sales_df.head(5).T),\n    (\"tail\", sales_df.tail(5).T),\n], cloud_env='Databricks'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9baf0f96-b2c2-489f-b636-1b34a358fb04"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["save_dataset(context, sales_df_processed, 'cleaned/sales')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c86db4f-ebe9-4bf7-ad67-84bcef1244f2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# 3. Generate Train, Validation and Test datasets"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1156b61e-e5d9-4013-ad68-f0cda112fa9f"}}},{"cell_type":"markdown","source":["- We split the data into train, test (optionally, also a validation dataset)\n- In this example, we are binning the target into 10 quantiles and then use a Stratified Shuffle to split the data.\n- See sklearn documentation on the various available splitters\n- https://scikit-learn.org/stable/modules/classes.html#splitter-classes\n- This will go into production code (training only)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2c27b0d-95c9-4c7a-a77f-7101a11f77c4"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80424228-a932-4f2b-a2d8-6029f49b7e96"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["from sklearn.model_selection import StratifiedShuffleSplit\nfrom ta_lib.core.api import custom_train_test_split  # helper function to customize splitting\nfrom scripts import *\n\nsplitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=context.random_seed)\nsales_df_train, sales_df_test = custom_train_test_split(sales_df_processed, splitter, by=binned_selling_price)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfcf6da6-8e87-4dd6-9eee-c3ab8a08b3d7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["target_col = \"unit_price\"\n\ntrain_X, train_y = (\n    sales_df_train\n    \n    # split the dataset to train and test\n    .get_features_targets(target_column_names=target_col)\n)\nsave_dataset(context, train_X, 'train/sales/features')\nsave_dataset(context, train_y, 'train/sales/target')\n\n\ntest_X, test_y = (\n    sales_df_test\n    \n    # split the dataset to train and test\n    .get_features_targets(target_column_names=target_col)\n)\nsave_dataset(context, test_X, 'test/sales/features')\nsave_dataset(context, test_y, 'test/sales/target')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1afefb10-0997-4b2b-aba4-55178a042f07"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"024fffae-d894-44a2-9f36-ff23fa759b4f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.7","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"02_data_processing","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2486657885092189}},"nbformat":4,"nbformat_minor":0}
